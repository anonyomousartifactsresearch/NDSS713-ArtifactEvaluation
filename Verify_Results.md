# **Reproducibility Guide: Verifying PANDORA Results**

This guide maps the experimental results generated by the PANDORA artifact to the specific Tables and Figures presented in the paper.

Important Note on F1-Scores:  
When verifying F1-scores in the CSV files, please examine both the "macro avg" and "weighted avg" rows. The values reported in the paper correspond to the best (highest) of these two averages, unless a specific class is mentioned.  
**Location:** All CSV results are located in the results/ directory.

## **1\. State-of-the-Art (SOTA) Comparison (Table III)**

*Paper Reference: Table III \- SOTA Comparison on CICIDS2017 (Few-Shot Adaptation)*

These experiments compare PANDORA against PTN-IDS across three scenarios (Sc.1, Sc.2, Sc.3).

| Paper Scenario | Artifact Directory | File to Verify | Metric Mapping |
| :---- | :---- | :---- | :---- |
| **Sc.1 (DDoS)** | results/pandora\_vs\_ptnids\_s1/ | report\_adapted\_model\_1\_shot\_on\_all\_attacks.csv | Look for **1-shot** row. |
|  |  | report\_adapted\_model\_5\_shot\_on\_all\_attacks.csv | Look for **5-shot** row. |
|  |  | report\_adapted\_model\_10\_shot\_on\_all\_attacks.csv | Look for **10-shot** row. |
| **Sc.2 (Web)** | results/pandora\_s2\_cicids2017/ | report\_adapted\_model\_1\_shot\_on\_all\_attacks.csv | Look for **1-shot** row. |
|  |  | report\_adapted\_model\_5\_shot\_on\_all\_attacks.csv | Look for **5-shot** row. |
| **Sc.3 (PortScan)** | results/pandora\_s3\_cicids2017/ | report\_adapted\_model\_1\_shot\_on\_all\_attacks.csv | Look for **1-shot** row. |
|  |  | report\_adapted\_model\_10\_shot\_on\_all\_attacks.csv | Look for **10-shot** row. |

**Verification Tip:** For the rows indicated above, check the **F1-score** column. Compare the **macro avg** and **weighted avg** values and take the higher one to match Table III.

## **2\. Distance Function Ablation (Table IV)**

*Paper Reference: Table IV \- Comparison of Distance Functions (PTN-IDS vs PANDORA)*

This experiment compares Euclidean, Manhattan, Cosine, and Wasserstein (PANDORA) metrics.

* **Artifact Directory:** results/loss\_ablation\_cicids2017/  
* **File:** ablation\_results.csv  
* **How to Verify:**  
  * Open ablation\_results.csv.  
  * Each row corresponds to a loss/distance configuration (e.g., Euclidean, Wasserstein/Full PMSD).  
  * Compare the **F1** values. **Note:** Ensure you compare the best of Macro/Weighted Average from the raw logs if ablation\_results.csv summarizes them differently.

## **3\. Domain Shift Robustness (Table V)**

*Paper Reference: Table V \- Performance Under Domain Shift (Train: CICIDS2017, Test: CICIDS2018)*

This evaluates the model trained on CICIDS2017 when applied to CICIDS2018.

* **Artifact Directory:** results/cicids2017\_domain\_shift/ or results/cicids2018\_zsl\_5\_shot/  
* **Files to Verify:**  
  * **Zero-Shot (Source):** results/cicids2018\_zsl\_5\_shot/report\_cicids2018\_test\_set.csv (Look for the initial performance before adaptation).  
  * **1-shot / 5-shot Adaptation:** results/cicids2017\_domain\_shift/report\_cicids2018\_adapted\_on\_1\_shot\_re\_scaled.csv.

## **4\. Architecture Ablation (Table VIII)**

*Paper Reference: Table VIII \- Ablation of Encoder Architecture (Mamba-MoE vs Transformer)*

Comparisons between the Mamba-MoE encoder and a standard Transformer.

| Dataset | Artifact Directory | File |
| :---- | :---- | :---- |
| **CICIDS2017** | results/arch\_ablation\_cicids2017/ | architecture\_ablation\_results.csv |
| **CICIoT2023** | results/arch\_ablation\_ciciot2023/ | architecture\_ablation\_results.csv |
| **TTDFIoT2025** | results/arch\_ablation\_ttdfiot2025/ | architecture\_ablation\_results.csv |

**How to Verify:** Open the CSV and compare the rows labeled Mamba-MoE vs Transformer. Compare the **F1** column (taking the best of Macro/Weighted Avg).

## **5\. PMSD Loss Ablation (Table VII)**

*Paper Reference: Table VII \- Ablation Study of the PMSD Loss Function*

Evaluates "Full PMSD", "Wasserstein only", "Triplet only", and "Euclidean only".

| Dataset | Artifact Directory | File |
| :---- | :---- | :---- |
| **CICIoT2023** | results/loss\_ablation\_ciciot2023/ | ablation\_results.csv |
| **TTDFIoT2025** | results/loss\_ablation\_ttdfiot/ | ablation\_results.csv |
| **CICIDS2017** | results/loss\_ablation\_cicids2017/ | ablation\_results.csv |

## **6\. Plots and Visualizations**

* **t-SNE Plots (Fig. 3):**  
  * Located in: results/pandora\_vs\_ptnids\_s1/tsne\_plot\_DDoS.png (and similar paths for other datasets).  
* **Training/Accuracy Plots:**  
  * Located in: results/pandora\_ttdfiot\_random/auc\_roc\_plot\_\*.png, results/pandora\_ciciot2023\_random/training\_plots\_\*.png.  
* **Feature Importance (Table IX):**  
  * Visualized in: results/\*/feature\_importance\_temporal\_\*.png and results/\*/feature\_importance\_volumetric\_\*.png.

## **7\. On-Device Performance (Table X)**

*Paper Reference: Table X \- Pandora On-Device Performance on Raspberry Pi*

* **Note:** If you ran the run\_pi\_benchmarks.sh script (or equivalent) on a Raspberry Pi, the logs would be generated in the root or a dedicated pi\_logs/ folder.  
* **For Reviewers running on x86/Cloud:** The inference times in results/\*/training\_history\_\*.csv (column time\_per\_epoch or similar) reflect the workstation performance, not the Raspberry Pi performance reported in Table X.